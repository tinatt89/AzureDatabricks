{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0603c85",
   "metadata": {},
   "source": [
    "### Import data as a df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57409630",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a04528",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('Bacis').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f6099e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the data as df\n",
    "df = spark.read.json('people.json')\n",
    "# show the df\n",
    "df.show()\n",
    "# show the schema of the df\n",
    "df.printSchema()\n",
    "# return a list of columns \n",
    "df.columns\n",
    "# return a df contains the statistic summary of the numeric columns in the df\n",
    "df.describe()\n",
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b04c0d7",
   "metadata": {},
   "source": [
    "### Define a schema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de7263a4",
   "metadata": {},
   "source": [
    "Sometimes the incoming data might not have a clear schema, so you can define/specify the schema by yourself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ea03ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructField, StructureType, IntegerType, StructureType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4d4411",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_schema = [StructureField('age',IntegerType(),True),\n",
    "               StructureField('name',StringType(),True)]\n",
    "final_struc = StructType(fields = data_scehma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec89813",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.json('people.json',schema = final_struc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "437223b8",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3551f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# return a column object\n",
    "df['age']\n",
    "# retrun a df\n",
    "df.select('age')\n",
    "# show the column\n",
    "df.select('age').show()\n",
    "# select mutiple columns\n",
    "df.select(['age','name'])\n",
    "df.select(['age','name']).show()\n",
    "# first two rows, return a list of row objects\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bced693",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a new column, doesn't impact the original df\n",
    "df.withColumn('newage',df['age']).show()\n",
    "# rename the column, impact the original df\n",
    "df.withColumnReanamed('age','new_age').show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d1eaa5a",
   "metadata": {},
   "source": [
    "### Intereact with SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a6737a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create/replace a temp view on the df \n",
    "df.createOrReplaceTempView('people')\n",
    "# use sql to interact with view\n",
    "results = spark.sql(\"select * From people\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b045eff",
   "metadata": {},
   "source": [
    "### Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ab6648",
   "metadata": {},
   "outputs": [],
   "source": [
    "# return all rows satisfying the condition\n",
    "df.filter(df['age'] < 20).show()\n",
    "df.filter(df['age'] == 20).show()\n",
    "# return selected rows and columns satisfying the condition\n",
    "df.filter(df['age'] < 20).select('name').show()\n",
    "\n",
    "# you can collect the result for later usage\n",
    "# returns a list of row objects\n",
    "results = df.filter(df['age'] < 20).select('name').collect()\n",
    "# convert to dict =>easier to use\n",
    "result = results[0].asDict()\n",
    "\n",
    "# Filtering on multiple conditions\n",
    "# Logical Operators and:&  or:pipe  not:~ \n",
    "# conditions need to be contained in brakets\n",
    "df.filter((df['age'] > 5 ) & (df['age'] < 20)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b4defa",
   "metadata": {},
   "source": [
    "### Joins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2735ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ab_inner = df_a.join(df_b, df_a.id == df_b.id, \"inner\")\n",
    "df_ab_outer = df_a.join(df_b, df_a.id == df_b.id, \"outer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a5d48fb",
   "metadata": {},
   "source": [
    "### Aggregate Function, GroupBy, and Window Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc339815",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import count,countDistinct, sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48d6168",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate Fucntions\n",
    "df.select(count(\"*\")).show()\n",
    "df.select(count(\"col_name\")).show()\n",
    "df.select(count(\"col_name\"),countDistinct(\"col_name\")).show()\n",
    "df.filter(df['name']=='Kim').select(count(\"col_name\"),countDistinct(\"col_name\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11cc67e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group By\n",
    "# groupby by a column, then aggregate, here only allows for one aggregation function\n",
    "df.groupBy('Column_Name').max().show()\n",
    "# to apply mutiple aggregation function, we need to use agg()\n",
    "df.groupBy('Column_Name').agg(sum(\"points\").alias(\"Total_points\"),count(\"ID\").alias(\"Total_numebr\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8053348a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import desc, rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75e4986",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Window Fucntions\n",
    "# Example: create a rank column based on partitioned by col and order by col_2\n",
    "# Define the window\n",
    "RankSpec = Window.partitionBy(\"col_1\").orderBy(desc(\"col_2\"))\n",
    "# Apply the window with funciton\n",
    "df.withColumn(\"Rank\",rank().over(RankSpec))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "977d7352",
   "metadata": {},
   "source": [
    "### Order by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d28f63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ascending\n",
    "df.orderBy('Column_Name').show()\n",
    "# Descending\n",
    "df.orderBy(df['Column_Name'].desc()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6963500",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e2ace4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import countDistinct, avg, stddev, format_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4196f2be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the average of a column\n",
    "df.select(avg('Column_Name')).show()\n",
    "# you can give the calcuated column an alias\n",
    "df.select(avg('Column_Name').alias('Average_value')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69466518",
   "metadata": {},
   "outputs": [],
   "source": [
    "# format the number, keep 2 decimal places\n",
    "df.select(format_number('Column_Nmae',2)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f595f1d",
   "metadata": {},
   "source": [
    "### Mssing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f30272f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping a row as long as it has a missing value\n",
    "df.na.drop().show()\n",
    "# Set a threshold for the number of non-missing values one row must have\n",
    "# otherwise it will be dropped\n",
    "df.na.drop(thresh = 2).show()\n",
    "\n",
    "# drop a row as long as there is a null value\n",
    "df.na.drop(how = 'any').show()\n",
    "# drop a row only when all of its value is null\n",
    "df.na.drop(how = 'all').show()\n",
    "# drop a row based on a subset of columns\n",
    "df.na.drop(subset = ['Column_Name']).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3b81d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill the missing value\n",
    "# it will fill the null cell \n",
    "# if the data type of 'fill value' matches the data type of the cell\n",
    "df.na.fill('Fill Value').show()\n",
    "\n",
    "df.na.fill('Zoe', subset=['name']).show()\n",
    "\n",
    "mean_val = df.select(mean(df['Sales'])).collect()\n",
    "mean_sales = mean_val[0][0]\n",
    "df.na.fill(mean_sales,subset=['Sales']).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
